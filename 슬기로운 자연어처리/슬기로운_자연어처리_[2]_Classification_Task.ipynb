{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "슬기로운_자연어처리_[2]_Classification_Task.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjvpuM5W2sFi",
        "outputId": "04acbfe9-1a85-44a7-d408-76004064584a"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-27 01:24:51--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-09-27 01:24:51--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-09-27 01:24:52--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.00MB/s    in 2m 48s  \n",
            "\n",
            "2021-09-27 01:27:40 (4.88 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tT0vTbcI-aJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f101b8e2-96f2-4707-8c4a-55b2bdf6f19e"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, LSTM\n",
        "from keras.layers import Embedding, Dense, Flatten, Input\n",
        "from keras.layers import add, concatenate\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.pooling import MaxPool1D\n",
        "from keras.models import Model\n",
        "from keras.preprocessing import text, sequence\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def load_data(train_dir, test_dir):\n",
        "    train = pd.read_csv(train_dir)\n",
        "    test = pd.read_csv(test_dir)\n",
        "\n",
        "    train, val = train_test_split(train, test_size=0.1, random_state=42)\n",
        "\n",
        "    train_x, train_y = train[\"storyline\"], train[\"label\"]\n",
        "    test_x, test_y = test[\"storyline\"], test[\"label\"]\n",
        "    val_x, val_y = val[\"storyline\"], val[\"label\"]\n",
        "\n",
        "    return train_x, train_y, test_x, test_y, val_x, val_y\n",
        "\n",
        "\n",
        "def pre_procissing(train_x, test_x, val_x):\n",
        "    CHARS_TO_REMOVE = r'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'\n",
        "\n",
        "    train_x = train_x.tolist()\n",
        "    test_x = test_x.tolist()\n",
        "    val_x = val_x.tolist()\n",
        "\n",
        "    tokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE)\n",
        "    tokenizer.fit_on_texts(train_x + test_x + val_x)  # Make dictionary\n",
        "\n",
        "    # Text match to dictionary.\n",
        "    train_x = tokenizer.texts_to_sequences(train_x)\n",
        "    test_x = tokenizer.texts_to_sequences(test_x)\n",
        "    val_x = tokenizer.texts_to_sequences(val_x)\n",
        "\n",
        "    temp_list = []\n",
        "    total_list = list(train_x) + list(test_x) + list(val_x)\n",
        "\n",
        "    for i in range(0, len(total_list)):\n",
        "        temp_list.append(len(total_list[i]))\n",
        "\n",
        "    max_len = max(temp_list)\n",
        "\n",
        "    train_x = sequence.pad_sequences(train_x, maxlen=max_len, padding='post')\n",
        "    test_x = sequence.pad_sequences(test_x, maxlen=max_len, padding='post')\n",
        "    val_x = sequence.pad_sequences(val_x, maxlen=max_len, padding='post')\n",
        "\n",
        "    return train_x, test_x, val_x, tokenizer\n",
        "\n",
        "\n",
        "def get_coefs(word, *arr):\n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "\n",
        "def load_embeddings(path):\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
        "\n",
        "\n",
        "def text_to_vector(word_index, path, word_dimension):\n",
        "    # If you change your embedding.pickle file, you must make new embedding.pickle file.\n",
        "    if os.path.isfile(\"embedding_binary.pickle\"):\n",
        "        with open(\"embedding_binary.pickle\", 'rb') as rotten_file:\n",
        "            embedding_matrix = pickle.load(rotten_file)\n",
        "\n",
        "    else:\n",
        "        embedding_index = load_embeddings(path)\n",
        "        embedding_matrix = np.zeros((len(word_index) + 1, 50))\n",
        "        for word, i in word_index.items():\n",
        "            try:\n",
        "                embedding_matrix[i] = embedding_index[word]\n",
        "            except KeyError:\n",
        "                pass\n",
        "\n",
        "        with open(\"embedding_binary.pickle\", 'wb') as handle:\n",
        "            pickle.dump(embedding_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def build_model(size, embedding_matrix):\n",
        "    ### Hyper Parameter\n",
        "    lstm_units = 128\n",
        "    hidden_units = 512\n",
        "\n",
        "    ### Model Architecture\n",
        "    input_layer = Input(shape=(size,))\n",
        "\n",
        "    embedding_layer = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "\n",
        "    lstm_layer = LSTM(128, return_sequences=True)(embedding_layer)\n",
        "    hidden_layer = Flatten()(lstm_layer)\n",
        "\n",
        "    output_layer = Dense(1, activation='sigmoid')(hidden_layer)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate(model, test_x, test_y):\n",
        "    prediction = model.predict(test_x)\n",
        "    y_pred = (prediction > 0.5)\n",
        "\n",
        "    accuracy = accuracy_score(test_y, y_pred)\n",
        "    print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "    print(classification_report(test_y, y_pred, target_names=[\"0\", \"1\"]))\n",
        "\n",
        "\n",
        "def main():\n",
        "    train_dir = \"train.csv\"\n",
        "    test_dir = \"test.csv\"\n",
        "    embedding_dir = \"glove.6B.50d.txt\"\n",
        "\n",
        "\n",
        "    ### Flow\n",
        "    train_x, train_y, test_x, test_y, val_x, val_y = load_data(train_dir, test_dir)\n",
        "    train_x, test_x, val_x, tokenizer = pre_procissing(train_x, test_x, val_x)\n",
        "    embedding_matrix = text_to_vector(tokenizer.word_index, embedding_dir, word_dimension=50)\n",
        "\n",
        "    model = build_model(train_x.shape[1], embedding_matrix)\n",
        "    model.fit(x=train_x, y=train_y, epochs=3, batch_size=64, validation_data=(val_x, val_y))\n",
        "\n",
        "    evaluate(model, test_x, test_y)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 258)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 258, 50)           474800    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 258, 128)          91648     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 33024)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 33025     \n",
            "=================================================================\n",
            "Total params: 599,473\n",
            "Trainable params: 124,673\n",
            "Non-trainable params: 474,800\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "13/13 [==============================] - 9s 485ms/step - loss: 0.7571 - accuracy: 0.4857 - val_loss: 0.6915 - val_accuracy: 0.5444\n",
            "Epoch 2/3\n",
            "13/13 [==============================] - 6s 442ms/step - loss: 0.6398 - accuracy: 0.6493 - val_loss: 0.7085 - val_accuracy: 0.5333\n",
            "Epoch 3/3\n",
            "13/13 [==============================] - 6s 442ms/step - loss: 0.5895 - accuracy: 0.7398 - val_loss: 0.7124 - val_accuracy: 0.5222\n",
            "Accuracy: 52.89%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.31      0.40       114\n",
            "           1       0.52      0.76      0.61       111\n",
            "\n",
            "    accuracy                           0.53       225\n",
            "   macro avg       0.54      0.53      0.51       225\n",
            "weighted avg       0.54      0.53      0.50       225\n",
            "\n"
          ]
        }
      ]
    }
  ]
}